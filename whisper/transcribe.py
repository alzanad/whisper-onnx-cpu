#!/usr/bin/env python3
"""
Ù…Ù„Ù Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù€ Whisper ONNX
ÙŠØ¯Ø¹Ù… CLI ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† Ù…Ù„Ù config.json

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    python transcribe.py                                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
    python transcribe.py -m tiny.en -l en -a audio.mp3     # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    python transcribe.py -m whisper-models/tiny.en          # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø³Ø§Ø± ÙƒØ§Ù…Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
    python transcribe.py -a my_audio.wav                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù…Ø¬Ù„Ø¯ audio/
    python transcribe.py --config custom_config.json       # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø®ØµØµ

Ù…Ù„Ø§Ø­Ø¸Ø§Øª:
    - Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØªÙØ¨Ø­Ø« ÙÙŠ Ù…Ø¬Ù„Ø¯ whisper-models/
    - Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© ØªÙØ¨Ø­Ø« ÙÙŠ Ù…Ø¬Ù„Ø¯ audio/ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙØ­Ø¯Ø¯ Ù…Ø³Ø§Ø± ÙƒØ§Ù…Ù„
    - Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªÙØ­ÙØ¸ ÙÙŠ Ù…Ø¬Ù„Ø¯ output/
"""

import os
import sys
import json
import argparse
import numpy as np
from pathlib import Path
from typing import List, Optional, Tuple, Union

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…Ø³Ø§Ø±
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from whisper.model import load_model, available_models
from whisper.audio import load_audio, pad_or_trim, log_mel_spectrogram, SAMPLE_RATE, N_FRAMES, HOP_LENGTH
from whisper.decoding import decode, detect_language, DecodingOptions, DecodingResult
from whisper.tokenizer import get_tokenizer, LANGUAGES, TO_LANGUAGE_CODE
from whisper.utils import exact_div, format_timestamp, write_txt, write_vtt, write_srt
from whisper import utils

def load_config(config_path: str = None) -> dict:
    """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
    if config_path is None:
        # Ø§Ù„Ø¨Ø­Ø« Ø£ÙˆÙ„Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠØŒ Ø«Ù… ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø£Ø¨
        config_path = current_dir / "config.json"
        if not config_path.exists():
            config_path = project_root / "config.json"
    else:
        config_path = Path(config_path)
    
    if not config_path.exists():
        print(f"âŒ Ø®Ø·Ø£: Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {config_path}")
        return {}
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„ØªÙŠ ØªØ¨Ø¯Ø£ Ø¨Ù€ _
        return {k: v for k, v in config.items() if not k.startswith('_')}
    except json.JSONDecodeError as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª: {e}")
        return {}

def list_audio_files(audio_dir: str) -> list:
    """Ø¹Ø±Ø¶ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©"""
    if not os.path.isabs(audio_dir):
        audio_dir_path = current_dir / audio_dir
    else:
        audio_dir_path = Path(audio_dir)
    if not audio_dir_path.exists():
        return []
    
    supported_extensions = {'.mp3', '.wav', '.flac', '.m4a', '.ogg', '.wma', '.aac'}
    audio_files = []
    
    for file_path in audio_dir_path.iterdir():
        if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
            audio_files.append(file_path.name)
    
    return sorted(audio_files)

def resolve_model_path(model_name: str, models_dir: str = "whisper-models") -> tuple:
    """Ø­Ù„ Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"""
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø³Ø§Ø±Ø§Ù‹ ÙƒØ§Ù…Ù„Ø§Ù‹
    if '/' in model_name or '\\' in model_name:
        model_path = Path(model_name)
        base_name = model_path.name
        model_dir = model_path.parent
    else:
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ø³Ù…Ø§Ù‹ ÙÙ‚Ø·
        base_name = model_name
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙŠ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Ù…Ø³Ø§Ø± Ù…Ø·Ù„Ù‚)
        if not os.path.isabs(models_dir):
            model_dir = current_dir / models_dir
        else:
            model_dir = Path(models_dir)
    
    encoder_path = model_dir / f"{base_name}_encoder_11.onnx"
    decoder_path = model_dir / f"{base_name}_decoder_11.onnx"
    
    return str(model_dir / base_name), base_name, encoder_path, decoder_path

def resolve_audio_path(audio_name: str, audio_dir: str = "audio") -> str:
    """Ø­Ù„ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ - ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ù…Ø®ØªÙ„ÙØ©"""
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø³Ø§Ø±Ø§Ù‹ ÙƒØ§Ù…Ù„Ø§Ù‹ØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡ ÙƒÙ…Ø§ Ù‡Ùˆ
    if '/' in audio_name or '\\' in audio_name or os.path.exists(audio_name):
        return audio_name
    
    # Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
    supported_extensions = ['.mp3', '.wav', '.flac', '.m4a', '.ogg', '.wma', '.aac']
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ø³Ù…Ø§Ù‹ ÙÙ‚Ø·ØŒ Ø§Ø¨Ø­Ø« ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØµÙˆØªÙŠØ§Øª
    if not os.path.isabs(audio_dir):
        audio_dir_path = current_dir / audio_dir
    else:
        audio_dir_path = Path(audio_dir)
    
    # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø£ÙˆÙ„Ø§Ù‹
    direct_path = audio_dir_path / audio_name
    if direct_path.exists():
        return str(direct_path)
    
    # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù„Ù‡ Ø§Ù…ØªØ¯Ø§Ø¯ØŒ Ø¬Ø±Ø¨ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    if '.' not in audio_name:
        for ext in supported_extensions:
            test_path = audio_dir_path / (audio_name + ext)
            if test_path.exists():
                return str(test_path)
    
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ ÙƒØ­Ù„ Ø£Ø®ÙŠØ±
    if os.path.exists(audio_name):
        return audio_name
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ
    if '.' not in audio_name:
        for ext in supported_extensions:
            test_path = audio_name + ext
            if os.path.exists(test_path):
                return test_path
    
    # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£ØµÙ„ÙŠ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø®Ø·Ø£ Ù„Ø§Ø­Ù‚Ø§Ù‹
    return audio_name

def transcribe(
    *,
    model,
    audio: Union[str, np.ndarray],
    verbose: Optional[bool] = None,
    temperature: Union[float, Tuple[float, ...]] = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
    compression_ratio_threshold: Optional[float] = 2.4,
    logprob_threshold: Optional[float] = -1.0,
    no_speech_threshold: Optional[float] = 0.6,
    condition_on_previous_text: bool = True,
    **decode_options,
):
    """
    Transcribe an audio file using Whisper
    """
    from typing import List, Optional, Tuple, Union
    
    mel: np.ndarray = log_mel_spectrogram(audio)

    if decode_options.get("language", None) is None:
        if verbose:
            print("ğŸ” ÙƒØ´Ù Ø§Ù„Ù„ØºØ©...")
        segment = pad_or_trim(mel, N_FRAMES)
        _, probs = model.detect_language(segment)
        decode_options["language"] = max(probs, key=probs.get)
        if verbose is not None:
            print(f"ğŸŒ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {LANGUAGES[decode_options['language']].title()}")

    mel = mel[np.newaxis, ...]
    language = decode_options["language"]
    task = decode_options.get("task", "transcribe")
    tokenizer = get_tokenizer(model.is_multilingual, language=language, task=task)

    def decode_with_fallback(segment: np.ndarray) -> List[DecodingResult]:
        temperatures = [temperature] if isinstance(temperature, (int, float)) else temperature
        kwargs = {**decode_options}
        t = temperatures[0]
        if t == 0:
            best_of = kwargs.pop("best_of", None)
        else:
            best_of = kwargs.get("best_of", None)

        options = DecodingOptions(**kwargs, temperature=t)
        results = model.decode(segment, options)

        kwargs.pop("beam_size", None)  # no beam search for t > 0
        kwargs.pop("patience", None)  # no patience for t > 0
        kwargs["best_of"] = best_of  # enable best_of for t > 0
        for t in temperatures[1:]:
            needs_fallback = [
                compression_ratio_threshold is not None
                and result.compression_ratio > compression_ratio_threshold
                or logprob_threshold is not None
                and result.avg_logprob < logprob_threshold
                for result in results
            ]
            if any(needs_fallback):
                options = DecodingOptions(**kwargs, temperature=t)
                retries = model.decode(segment[needs_fallback], options)
                for retry_index, original_index in enumerate(np.nonzero(needs_fallback)[0]):
                    results[original_index] = retries[retry_index]

        return results

    seek = 0
    input_stride = exact_div(N_FRAMES, model.dims.n_audio_ctx)  # mel frames per output token: 2
    time_precision = (input_stride * HOP_LENGTH / SAMPLE_RATE)  # time per output token: 0.02 (seconds)
    all_tokens = []
    all_segments = []
    prompt_reset_since = 0

    initial_prompt = decode_options.pop("initial_prompt", None) or []
    if initial_prompt:
        initial_prompt = tokenizer.encode(" " + initial_prompt.strip())
        all_tokens.extend(initial_prompt)

    def add_segment(*, start: float, end: float, text_tokens: np.ndarray, result: DecodingResult):
        text = tokenizer.decode([token for token in text_tokens if token < tokenizer.eot])
        if len(text.strip()) == 0:  # skip empty text output
            return

        all_segments.append({
            "id": len(all_segments),
            "seek": seek,
            "start": start,
            "end": end,
            "text": text,
            "tokens": result.tokens,
            "temperature": result.temperature,
            "avg_logprob": result.avg_logprob,
            "compression_ratio": result.compression_ratio,
            "no_speech_prob": result.no_speech_prob,
        })
        if verbose:
            print(f"[{format_timestamp(start)} --> {format_timestamp(end)}] {text}", flush=True)

    # show the progress bar when verbose is False (otherwise the transcribed text will be printed)
    num_frames = mel.shape[-1]
    previous_seek_value = seek

    while seek < num_frames:
        timestamp_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)
        segment = pad_or_trim(mel[:, :, seek:], N_FRAMES)
        segment_duration = segment.shape[-1] * HOP_LENGTH / SAMPLE_RATE

        decode_options["prompt"] = all_tokens[prompt_reset_since:]
        result = decode_with_fallback(segment)[0]
        tokens = result.tokens

        if no_speech_threshold is not None:
            # no voice activity check
            should_skip = result.no_speech_prob > no_speech_threshold
            if logprob_threshold is not None and result.avg_logprob > logprob_threshold:
                # don't skip if the logprob is high enough, despite the no_speech_prob
                should_skip = False

            if should_skip:
                seek += segment.shape[-1]  # fast-forward to the next segment boundary
                continue

        timestamp_tokens: np.ndarray = np.greater_equal(tokens, tokenizer.timestamp_begin)
        consecutive = np.add(np.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0], 1)
        if len(consecutive) > 0:  # if the output contains two consecutive timestamp tokens
            last_slice = 0
            for current_slice in consecutive:
                sliced_tokens = tokens[last_slice:current_slice]
                start_timestamp_position = (sliced_tokens[0] - tokenizer.timestamp_begin)
                end_timestamp_position = (sliced_tokens[-1] - tokenizer.timestamp_begin)
                add_segment(
                    start=timestamp_offset + start_timestamp_position * time_precision,
                    end=timestamp_offset + end_timestamp_position * time_precision,
                    text_tokens=sliced_tokens[1:-1],
                    result=result,
                )
                last_slice = current_slice
            last_timestamp_position = (tokens[last_slice - 1] - tokenizer.timestamp_begin)
            seek += last_timestamp_position * input_stride
            all_tokens.extend(list(tokens[: last_slice + 1]))
        else:
            duration = segment_duration
            tokens = np.asarray(tokens) if isinstance(tokens, list) else tokens
            timestamps = tokens[np.ravel_multi_index(np.nonzero(timestamp_tokens), timestamp_tokens.shape)]
            if len(timestamps) > 0:
                # no consecutive timestamps but it has a timestamp; use the last one.
                # single timestamp at the end means no speech after the last timestamp.
                last_timestamp_position = timestamps[-1] - tokenizer.timestamp_begin
                duration = last_timestamp_position * time_precision

            add_segment(
                start=timestamp_offset,
                end=timestamp_offset + duration,
                text_tokens=tokens,
                result=result,
            )

            seek += segment.shape[-1]
            all_tokens.extend(list(tokens))

        if not condition_on_previous_text or result.temperature > 0.5:
            # do not feed the prompt tokens if a high temperature was used
            prompt_reset_since = len(all_tokens)

    return dict(text=tokenizer.decode(all_tokens[len(initial_prompt):]), segments=all_segments, language=language)
def create_output_dir(output_dir: str):
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹"""
    os.makedirs(output_dir, exist_ok=True)

def save_output(result, audio_path: str, output_dir: str, formats: list):
    """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø§Ù„ØµÙŠØº Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
    audio_name = Path(audio_path).stem
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ ÙˆØ§Ù„Ù€ segments
    if isinstance(result, dict):
        text = result.get("text", "")
        segments = result.get("segments", [])
    elif hasattr(result, 'text'):
        text = result.text
        segments = []
    else:
        text = str(result)
        segments = []
    
    # Ø¥Ù†Ø´Ø§Ø¡ segments Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ØªØ§Ø­Ø§Ù‹
    if not segments and text:
        segments = [{"start": 0.0, "end": 30.0, "text": text}]
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø§Ø¯ÙŠ
    if "txt" in formats:
        txt_path = Path(output_dir) / f"{audio_name}.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            write_txt(segments, file=f)
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Øµ: {txt_path}")
    
    # Ø­ÙØ¸ VTT
    if "vtt" in formats:
        vtt_path = Path(output_dir) / f"{audio_name}.vtt"
        with open(vtt_path, 'w', encoding='utf-8') as f:
            write_vtt(segments, file=f)
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ VTT: {vtt_path}")
    
    # Ø­ÙØ¸ SRT  
    if "srt" in formats:
        srt_path = Path(output_dir) / f"{audio_name}.srt"
        with open(srt_path, 'w', encoding='utf-8') as f:
            write_srt(segments, file=f)
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ SRT: {srt_path}")

def main():
    parser = argparse.ArgumentParser(description="Whisper ONNX Transcription Tool")
    parser.add_argument("-m", "--model", type=str, help="Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ùˆ Ù…Ø³Ø§Ø±Ù‡")
    parser.add_argument("-l", "--language", type=str, help="Ø±Ù…Ø² Ø§Ù„Ù„ØºØ© (Ù…Ø«Ù„: en, ar)")
    parser.add_argument("-a", "--audio", type=str, help="Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ Ø£Ùˆ Ø§Ø³Ù…Ù‡ ÙÙ‚Ø·")
    parser.add_argument("-o", "--output", type=str, help="Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬")
    parser.add_argument("--config", type=str, help="Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØµØµ")
    parser.add_argument("--list-models", action="store_true", help="Ø¹Ø±Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©")
    parser.add_argument("--list-audio", action="store_true", help="Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©")
    parser.add_argument("--formats", nargs="+", choices=["txt", "vtt", "srt"], help="ØµÙŠØº Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬")
    parser.add_argument("-v", "--verbose", action="store_true", help="Ø¹Ø±Ø¶ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØ«Ø±")
    
    args = parser.parse_args()
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£ÙˆÙ„Ø§Ù‹ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
    config = load_config(args.config)
    audio_dir = config.get("audio_directory", "audio")
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
    if args.list_models:
        print("ğŸ“‹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:")
        for model in available_models():
            print(f"   - {model}")
        return
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©
    if args.list_audio:
        audio_files = list_audio_files(audio_dir)
        if audio_files:
            print(f"ğŸµ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ '{audio_dir}':")
            for audio_file in audio_files:
                print(f"   - {audio_file}")
            print(f"\nğŸ’¡ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…: python transcribe.py -a filename")
        else:
            print(f"ğŸ“­ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ© ÙÙŠ '{audio_dir}'")
            print(f"ğŸ’¡ Ø¶Ø¹ Ù…Ù„ÙØ§ØªÙƒ Ø§Ù„ØµÙˆØªÙŠØ© ÙÙŠ Ù…Ø¬Ù„Ø¯ '{audio_dir}' Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„")
        return
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø¹ Ù…Ø¹Ø§Ù…Ù„Ø§Øª CLI
    model_name = args.model or config.get("default_model", "tiny.en")
    language = args.language or config.get("default_language", "en")
    audio_name = args.audio or config.get("default_audio", "audio.mp3")
    output_dir = args.output or config.get("output_directory", "output")
    models_dir = config.get("models_directory", "whisper-models")
    audio_dir = config.get("audio_directory", "audio")
    output_formats = args.formats or config.get("output_formats", ["txt"])
    verbose = args.verbose or config.get("verbose", False)
    
    # Ø­Ù„ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
    audio_path = resolve_audio_path(audio_name, audio_dir)
    
    if verbose:
        print(f"ğŸ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_name}")
        print(f"ğŸŒ Ø§Ù„Ù„ØºØ©: {language}")
        print(f"ğŸµ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ: {audio_name} -> {audio_path}")
        print(f"ğŸ“ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØµÙˆØªÙŠØ§Øª: {audio_dir}")
        print(f"ğŸ“ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {output_dir}")
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
    if not os.path.exists(audio_path):
        print(f"âŒ Ø®Ø·Ø£: Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {audio_path}")
        if audio_name != audio_path:
            print(f"ğŸ” ØªÙ… Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† '{audio_name}' ÙÙŠ:")
            print(f"   - Ù…Ø¬Ù„Ø¯ Ø§Ù„ØµÙˆØªÙŠØ§Øª: {audio_dir}/")
            print(f"   - Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ")
            print(f"ğŸ’¡ ØªÙ„Ù…ÙŠØ­: ÙŠÙ…ÙƒÙ†Ùƒ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙÙŠ Ù…Ø¬Ù„Ø¯ '{audio_dir}' ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù ÙÙ‚Ø·")
            print(f"ğŸ“ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©: .mp3, .wav, .flac, .m4a, .ogg, .wma, .aac")
        return
    
    # Ø­Ù„ Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model_path, base_model_name, encoder_path, decoder_path = resolve_model_path(model_name, models_dir)
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    if not encoder_path.exists() or not decoder_path.exists():
        print(f"âŒ Ø®Ø·Ø£: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        print(f"ğŸ“ Ù…Ù„ÙØ§Øª Ù…Ø·Ù„ÙˆØ¨Ø©:")
        print(f"   - {encoder_path}")
        print(f"   - {decoder_path}")
        print(f"ğŸ“¥ ÙŠØ¬Ø¨ ØªÙ†Ø²ÙŠÙ„ Ù‡Ø°ÙŠÙ† Ø§Ù„Ù…Ù„ÙÙŠÙ† ÙˆÙˆØ¶Ø¹Ù‡Ù…Ø§ ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨")
        print(f"ğŸ”— Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø¯Ø§Ø© Ø§Ù„ØªÙ†Ø²ÙŠÙ„: python download_models/download_models.py --model {base_model_name}")
        return
    
    if verbose:
        print(f"ğŸ“‚ Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_path}")
        print(f"ğŸ”§ Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {base_model_name}")
    
    try:
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        print(f"ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {base_model_name}")
        model = load_model(model_path)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª
        print(f"ğŸµ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ: {audio_path}")
        audio = load_audio(audio_path)
        audio = pad_or_trim(audio)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ mel spectrogram
        mel = log_mel_spectrogram(audio)
        
        # Ø§Ù„Ù†Ø³Ø® Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø¯Ø«Ø©
        print("âš¡ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù†Ø³Ø®...")
        result = transcribe(
            model=model,
            audio=audio_path,
            language=language if language != "auto" else None,
            verbose=verbose
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        create_output_dir(output_dir)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        save_output(result, audio_path, output_dir, output_formats)
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
        print("\nğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬:")
        print("=" * 50)
        print(result["text"])
        print("=" * 50)
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù€ segments Ø¥Ø°Ø§ ÙƒØ§Ù† verbose
        if verbose and result.get("segments"):
            print(f"\nğŸ“Š ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(result['segments'])} Ù…Ù‚Ø·Ø¹:")
            for segment in result["segments"]:
                start_time = format_timestamp(segment["start"])
                end_time = format_timestamp(segment["end"])
                print(f"   [{start_time} --> {end_time}] {segment['text']}")
        
        print(f"âœ… ØªÙ…Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©: {e}")
        if verbose:
            import traceback
            traceback.print_exc()

def cli():
    """Ù†Ù‚Ø·Ø© Ø¯Ø®ÙˆÙ„ ÙˆØ§Ø¬Ù‡Ø© Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±"""
    main()

if __name__ == "__main__":
    main()